\chapter{Scoring}
\label{app:C}


Scoring in PREGO is used to answer the questions:
\begin{itemize}
   \item Which associations are more thrustworthy?
   \item Which associations are more relevant to the user's query?
\end{itemize}

Relevant, informative, and probable associations are presented to the user through the three channels that were discussed previously. 
Each channel has its own scoring scheme for the associations it contains and all of them are fit in the interval $(0,5]$ to maintain consistency. 
The values of the score are visually shown as stars. 
The Genome Annotation and Isolates channel has fixed values of scores depending on the resource because Genome Annotation is straightforward, and the microbe id is known a priori. 
On the other hand, Environmental Samples channel data are based on samples, which contain metagenomes and OTU tables. 
Thus, it has two levels of organization, microbes with metadata, and sample identifiers. Each association of two entities is scored based on the number of samples they co-occur. 
A Literature channel scoring scheme is based on the co-mention of a pair of entities in each document, paragraph, and sentence. The differences in the nature of data require different scoring schemes in these channels.
The contingency table (Table A1) of two random variables, $X$ and $Y$ are the starting point for the calculation of scores. The term $X = 1$ might be a specific NCBI id and $Y = 1$ a ENVO term. 
The $c_{1,1}$ is the number of instances that two terms of $X = 1$ and $Y = 1$ are co-occurring, i.e., the joint frequency. 
The marginals are the $c_{1,.}$ and $c{.,1}$ for $x$ and $y$, respectively, which are the backgrounds for each entity type. 
Different handling of these frequencies leads to different measures. 
There is not a perfect scoring scheme, just the one that works best on a particular instance. 
Consequently, scoring attributes require testing different measures and their parameters.



\begin{table}[h]
   \centering
   \begin{tabular}{c|llll}
    & \multicolumn{4}{l}{Y = y} \\ \cline{2-5} 
   \multirow{4}{*}{X = x} &  & Yes & No & Total \\ \cline{3-5} 
    & \multicolumn{1}{l|}{Yes} & $c_{x,y}$ & $c_{x,0}$ & $c_{x,.}$ \\
    & \multicolumn{1}{l|}{No} & $c_{0,y}$ & $c_{0,0}$ & $c_{0,.}$ \\
    & \multicolumn{1}{l|}{Total} & $c_{.,y}$ & $c_{.,0}$ & $c_{.,.}$
   \end{tabular}
   \caption[PREGO contingency table between two terms]{Contingency table of co-occurrences between entities $X = x$ and $Y = y$. 
   This is the basic structure for all scoring schemes. $c_{x,y}$ is the count of the co-occurrence of these entities. $c_{x,.}$ is the count of the $x$ with all the entities of $Y$ type (e.g., Molecular function). Conversely, $c_{.,y}$ is the count of $y$ with all the entities of $X$ type (e.g., taxonomy}
   \label{table:pregoA1}
\end{table}


\section*{Literature Channel}

Scoring in the Literature channel is implemented as in STRING 9.1 \citep{franceschini2012string} and COMPARTMENTS \citep{binder2014compartments}, where the text mining method uses a three-step scoring scheme. 
First, for each co-mention/co-occurrence between entities (e.g., Methanosarcina mazei with Sulfur carrier activity), a weighted count is calculated because of the complexity of the text.  


\begin{equation}
   c_{x,y} = \sum_{k=1}^{n}{w_d \delta_{dk}(x,y) +w_p \delta_{p,k}(x,y) + w_s \delta_{sk}(x,y)}
   \label{eq:prego-score-1}
\end{equation}



Different weights are used for each part of the document ($k$) for which both entities have been co-mentioned, $w_d = 1$ for the weight for the whole document level, $w_p = 2$ for the weight of the paragraph level, and $w_s = 0.2$ for the same sentence weight. 
Additionally, the delta functions are one (Equation~\ref{eq:prego-score-1}) in cases the co-mention exists, zero otherwise. Thus, the weighted count becomes higher as the entities are mentioned in the same paragraph and even higher when in the same sentence.
Subsequently, the co-occurrence score is calculated as follows:

\begin{equation}
   score_{x,y} = c_{x,y}^a (\frac{c_{x,y} c_{.,.}}{c_{x, .}c_{.,y}})^{1-a}
   \label{eq:prego-score-2}
\end{equation}
   


where $a = 0.6$ is a weighting factor, and the $c_{x,.}$, $c_{.,.}$, 
$c_{.,y}$ are the weighted counts as shown in Table~\ref{table:pregoA1} estimated using the same Equation~\ref{eq:prego-score-2}. 
This value of the weighting factor has been chosen because it has been optimized and benchmarked in various applications of text mining [34,70,71]. 
The value of Equation~\ref{eq:prego-score-2} is sensitive to the increasing size of the number of documents (MEDLINE PubMedâ€”PMC OA).
Therefore, to obtain a more robust measure, the value of the score is transformed to $z$-score. 
This transformation is elaborated in detail in the COMPARTMENTS resource \citep{binder2014compartments}. 
Finally, the confidence score is the $z$-score divided by two. Cases in which the scores exceed the (0,4] interval are capped to a maximum of 4 to reflect the uncertainty of the text mining pipeline.

\section*{Environmental Samples Channel}

Data from environmental samples are OTU tables and metagenomes. Thus, for each entity x, the number of samples is calculated as the background and a number of samples of the associated entity (metadata background) c.,y (see Table A1). Each association between entities x, y has a number of samples, cx,y that they co-occur. Note that each resource is independent and the scoring scheme is applied to its entities. This means that the same association can appear in multiple resources with different scores. The score is calculated with the following formula:

\begin{equation}
   score_{x,y} = 2.0*\sqrt{\frac{c_{x,y}}{c_{.,y}}}^{ \:a}
\end{equation}


This score is asymmetric because the denominator is the marginal of the associated entity. 
Thus, the score decreases as the marginal of $y$ is increasing, i.e., the number of samples that $y$ is found. 
On the other hand, it promotes associations in which the number of samples of the association are similar to the marginal of $y$. 
The exponents on the numerator and denominator equal to $0.5$ and 
to $0.1$, respectively, in order to reduce the rapid increase of score.
Lastly, the value of the score is capped in the range $(0,4]$.


